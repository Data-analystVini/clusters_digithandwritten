# -*- coding: utf-8 -*-
"""Kmeans_digits_handwritten.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IoESg1SY3HqJgZXL7Mfz0WUz5KxqgzVe
"""

import numpy as np

from sklearn.datasets import load_digits

data, labels = load_digits(return_X_y=True)
(n_samples, n_features), n_digits = data.shape, np.unique(labels).size

print(f"# digits: {n_digits}; # samples: {n_samples}; # features {n_features}")

"""digits from 0 to 9
Shorthand

full name

homo

homogeneity score

compl

completeness score

v-meas

V measure

ARI

adjusted Rand index

AMI

adjusted mutual information

silhouette

silhouette coeffecient

'''Time taken for clustering.
Inertia (a measure of how well the clusters fit the data).
Various clustering metrics to evaluate the quality of clustering'''
"""

from time import time

from sklearn import metrics#Imports various clustering metrics (e.g., homogeneity score, silhouette score) for evaluating the clustering performance.
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler# Imports StandardScaler, which standardizes features by removing the mean and scaling to unit variance.
'''Automatic Workflow Execution: When a pipeline is created, it sequentially applies each step to the data automatically when you
call .fit() or .predict() on the pipeline. For example, if you include scaling and a model in your pipeline, the scaling will happen
 before fitting or predicting with the model.'''

def bench_k_means(kmeans, name, data, labels):


    """Benchmark to evaluate the KMeans initialization methods.

    Parameters
    ----------
    kmeans : KMeans instance
        A :class:`~sklearn.cluster.KMeans` instance with the initialization
        already set.
    name : str
        Name given to the strategy. It will be used to show the results in a
        table.
    data : ndarray of shape (n_samples, n_features)
        The data to cluster.
    labels : ndarray of shape (n_samples,)
        The labels used to compute the clustering metrics which requires some
        supervision.True labels used to compute the clustering metrics for evaluation
    """
    t0 = time()
    estimator = make_pipeline(StandardScaler(), kmeans).fit(data)
    fit_time = time() - t0

    #metrics collection
    '''Inertia: estimator[-1].inertia_ returns the inertia from the kmeans model (last step in the pipeline).'''
    results = [name, fit_time, estimator[-1].inertia_]

    # Define the metrics which require only the true labels and estimator
    # labels
    clustering_metrics = [
        metrics.homogeneity_score,
        metrics.completeness_score,
        metrics.v_measure_score,
        metrics.adjusted_rand_score,
        metrics.adjusted_mutual_info_score,
    ]
    ''' homogeneity assesses if clusters contain only members of a single class, and completeness checks if all members of a given class are assigned to the same cluster.'''
    #each metric is computed with
    results += [m(labels, estimator[-1].labels_) for m in clustering_metrics]

    # The silhouette score requires the full dataset
    results += [
        metrics.silhouette_score(
            data,
            estimator[-1].labels_,
            metric="euclidean",
            sample_size=300,
        )
    ]

    # Show the results
    formatter_result = (
        "{:9s}\t{:.3f}s\t{:.0f}\t{:.3f}\t{:.3f}\t{:.3f}\t{:.3f}\t{:.3f}\t{:.3f}"
    )
    print(formatter_result.format(*results))

"""This formats and prints the results in a readable table. Each row displays:

Strategy name
Fit time
Inertia
Homogeneity, completeness, V-measure, adjusted Rand index, adjusted mutual info, and silhouette scores.
"""

from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

print(82 * "_")#this is a line seperator
print("init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\tsilhouette")

'''init=k-means++: This is a popular initialization method that spreads initial centroids to improve the convergence speed and stability of KMeans.
n_clusters=n_digits: Sets the number of clusters to the number of unique classes (or digits) in data.
n_init=4: The algorithm will be run 4 times with different centroid seeds, keeping the best result.
random_state=0: Ensures reproducibility by setting a seed for randomness.
bench_k_means is then called to run and time this setup, storing and printing the results.'''

kmeans = KMeans(init="k-means++", n_clusters=n_digits, n_init=4, random_state=0)
bench_k_means(kmeans=kmeans, name="k-means++", data=data, labels=labels)

'''init="random": This initialization selects random points from the data as initial centroids. While this method is simpler, it may lead to slower convergence or poorer clustering quality than k-means++.
Other parameters (n_clusters, n_init, and random_state) remain the same as the previous run.
Again, bench_k_means is called to evaluate and print the metrics for this method.
'''
kmeans = KMeans(init="random", n_clusters=n_digits, n_init=4, random_state=0)
bench_k_means(kmeans=kmeans, name="random", data=data, labels=labels)

'''PCA for Initialization: Here, Principal Component Analysis (PCA) is used to extract n_digits components from the data. PCA identifies orthogonal directions (components)
that capture the most variance, so initializing centroids with these components can sometimes yield better clustering results.
init=pca.components_: The initial centroids for KMeans are set to the PCA components.
n_init=1: Since PCA-based initialization is deterministic, thereâ€™s no need for multiple runs.
This setup is then passed to bench_k_means, which outputs metrics for the PCA-based initialization strategy.'''

pca = PCA(n_components=n_digits).fit(data)
kmeans = KMeans(init=pca.components_, n_clusters=n_digits, n_init=1)
bench_k_means(kmeans=kmeans, name="PCA-based", data=data, labels=labels)

print(82 * "_")

"""k-means++ initialization (faster convergence and generally good results).
random initialization (simpler but sometimes less effective).
PCA-based initialization (leverages PCA to create informed starting centroids).
The bench_k_means function evaluates and prints results for each setup, making it easy to compare the initialization strategies based on performance metrics.
"""

import matplotlib.pyplot as plt
#dimensionality reduction
'''PCA(n_components=2): Reduces the dataset to two principal components, making it suitable for 2D visualization.
.fit_transform(data): Fits PCA to the data and transforms it to the new 2D space, storing the result in reduced_data.
'''
reduced_data = PCA(n_components=2).fit_transform(data)
kmeans = KMeans(init="k-means++", n_clusters=n_digits, n_init=4)
kmeans.fit(reduced_data)

# Step size of the mesh. Decrease to increase the quality of the VQ.
h = 0.02  # point in the mesh [x_min, x_max]x[y_min, y_max].

# Plot the decision boundary. For that, we will assign a color to each
'''h = 0.02: Sets the mesh step size (smaller values provide finer detail).
x_min, x_max, y_min, y_max: Define the plot boundaries based on the minimum and maximum values in reduced_data, with some padding.
np.meshgrid: Creates a grid (xx and yy) of points within the specified boundaries.
'''
x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1
y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# Obtain labels for each point in mesh. Use last trained model.
'''np.c_[xx.ravel(), yy.ravel()]: Flattens xx and yy, then combines them to create an array of coordinate points.
kmeans.predict(): Assigns each point in the mesh grid to a cluster based on the KMeans model.
Z: Stores the cluster labels for each point in the mesh grid.'''
Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])

#reshaping the decision boundary
# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.figure(1)
plt.clf()
plt.imshow(
    Z,## Creates a color plot of the decision boundaries, where each cluster region is represented by a different color,Reshapes Z to match the grid's shape for plotting.
    interpolation="nearest",
    extent=(xx.min(), xx.max(), yy.min(), yy.max()),
    cmap=plt.cm.Paired,#Sets a colormap for distinguishing clusters.
    aspect="auto",
    origin="lower",
)


#plotting the datapoints and clustering centroids
'''Data Points: Plots the data points in reduced_data as small black dots with markersize=2.
Cluster Centroids: Plots the KMeans centroids as large white "X" markers to clearly mark their positions.'''
plt.plot(reduced_data[:, 0], reduced_data[:, 1], "k.", markersize=2)
# Plot the centroids as a white X
centroids = kmeans.cluster_centers_
plt.scatter(
    centroids[:, 0],
    centroids[:, 1],
    marker="x",
    s=169,
    linewidths=3,
    color="w",
    zorder=10,
)
#add titles and final display

plt.title(
    "K-means clustering on the digits dataset (PCA-reduced data)\n"
    "Centroids are marked with white cross"
)
plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)#Sets the x and y limits to match x_min, x_max, y_min, and y_max.
'''Hides Ticks: Removes the tick marks on both axes for a cleaner visualization.'''
plt.xticks(())
plt.yticks(())
plt.show()

